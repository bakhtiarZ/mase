
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         117     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       640 MACs
fwd flops per GPU:                                                      1.7 K   
fwd flops of model = fwd flops per GPU * mp_size:                       1.7 K   
fwd latency:                                                            698.8 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.44 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'JSC_Tiny': '117'}
    MACs        - {'JSC_Tiny': '640 MACs'}
    fwd latency - {'JSC_Tiny': '698.8 us'}
depth 1:
    params      - {'Sequential': '117'}
    MACs        - {'Sequential': '640 MACs'}
    fwd latency - {'Sequential': '626.09 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

JSC_Tiny(
  117 = 100% Params, 640 MACs = 100% MACs, 698.8 us = 100% latency, 2.44 MFLOPS
  (seq_blocks): Sequential(
    117 = 100% Params, 640 MACs = 100% MACs, 626.09 us = 89.59% latency, 2.72 MFLOPS
    (0): BatchNorm1d(32 = 27.35% Params, 0 MACs = 0% MACs, 111.82 us = 16% latency, 2.29 MFLOPS, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 139.24 us = 19.92% latency, 919.3 KFLOPS, inplace=True)
    (2): Linear(85 = 72.65% Params, 640 MACs = 100% MACs, 135.42 us = 19.38% latency, 9.45 MFLOPS, in_features=16, out_features=5, bias=True)
    (3): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 9.72% latency, 588.67 KFLOPS, inplace=True)
  )
)
------------------------------------------------------------------------------
